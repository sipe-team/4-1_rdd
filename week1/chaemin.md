# 회고록
## 실시간 에러 알림 시스템

### 기존 시스템 한계

기존 시스템에 대한 프로세스를 3단계로 요약하면 다음과 같습니다.

1. 시스템에서 예외가 발생하면 실시간으로 DB에 예외 정보들을 저장합니다.
2. 스케줄러가 배치 시스템을 주기적으로 호출하게 되고, 배치 시스템은 DB에 최근 5분 이내에 발생한 예외 정보들이 존재하는지 확인합니다.
3. DB에 최근 5분 이내에 발생한 예외 정보들이 존재한다면 Slack 모니터링 채널로 알림을 전송하게 됩니다.

이러한 기존 시스템은 겉으로 보기엔 괜찮아 보이지만 다음과 같은 단점이 존재합니다.

- 5분 스케줄링 배치로 동작하기 때문에 완전 실시간 에러 알림을 받을 수 없습니다.
- 스케줄러, 배치 서버, DB 중에 한 군데 이상 장애가 발생한다면 에러에 대한 알림을 받을 수 없습니다.
- 동일한 에러가 10분 이상 지속된다면 장애가 해소되기 전까지 에러 알림이 지속적으로 발생합니다.

### 개선점 도출

기존 시스템의 단점으로 보았을 때 어느 한 군데라도 제대로 동작하지 않는다면 에러에 대한 알림을 받을 수 없게 되고, 또한 알림을 받지 못하여 신속한 장애 대응이 불가피하게 됩니다.

기존 시스템은 스케줄러, 배치 서버, DB 중에 한 군데 이상 장애가 발생하면 에러에 대한 알림을 받을 수 없었습니다. 이에 대한 영향도를 최소화하기 위해서 AOP & Redis 기반의 에러 알림 시스템을 구축하였는데요, 그림을 보면서 이야기해 보겠습니다.

![](https://miro.medium.com/v2/resize:fit:1222/1*tugN8UlGOCQaZdjn5Oc6Ag.png)

시스템 흐름이 왜 3개인지 궁금해하실 수 있습니다. 3가지 흐름을 표현한 이유는 책임 연쇄 패턴(Chain Of Responsibility Pattern)을 사용해서 영향도를 최소화하여 로직을 구현했기 때문인데요, 시스템 흐름은 다음과 같이 이야기할 수 있습니다.

> _Chain Of Responsibility Pattern: 책임 연쇄 패턴(Chain Of Responsibility Pattern)은_ **_하나의 요청을 둘 이상의 객체에서 처리할 수 있게_** **_연쇄적으로 연결_**_하는 패턴입니다._

1. 시스템에서 예외가 발생하면 Redis에서 동일한 예외 정보가 존재하는지 판단합니다.

- 동일한 예외 정보가 존재 X: 새로운 예외로 판단하고 Redis에 TTL 5분을 설정 후 예외 정보를 삽입하고 알림을 전송합니다. (이때, 이 예외의 발생 건수는 1로 설정하고 삽입합니다.)
- 동일한 예외 정보가 존재 O: 최근 5분 또는 30분 이내에 중복으로 발생한 예외로 판단하고 Redis에 TTL 30분을 설정 후 예외 정보를 삽입하고 알림을 전송합니다. (이때, 이 예외의 발생 건수 +1로 설정하고 삽입하고, 기존 발생 건수가 5 초과라면 알림을 전송하지 않습니다.)

2. Redis 장애가 발생하여 더 이상 Redis를 사용할 수 없다면 책임은 RDB로 전가됩니다. 동일하게 시스템에서 예외가 발생하면 RDB에서 최근 30분 이내에 동일한 예외 정보가 존재하는지 판단합니다.

> Redis에 저장된 정보는 주기적으로 DB로 동기화됩니다.

- 동일한 예외 정보가 존재 X: 새로운 예외로 판단하고 RDB에 예외 정보를 삽입하고 알림을 전송합니다. (이때, 이 예외의 발생건수는 1로 설정하고 삽입합니다.)
- 동일한 예외 정보가 존재 O: 최근 5분 이내에 중복으로 발생한 예외로 판단하고 RDB에 예외 정보를 삽입하고 알림을 전송합니다. (이때, 이 예외의 발생 건수 +1로 설정하고 삽입하고, 기존 발생 건수가 5 초과라면 알림을 전송하지 않습니다.)

3. Redis와 RDB 모두 장애가 발생하여 더 이상 데이터 시스템에 의존이 불가능하다면 장애가 발생하면 무조건 알림을 전송하게 됩니다.

이렇게 기존 시스템의 한계를 극복하고 다음과 같은 개선점을 도출할 수 있었습니다.

1. 장애에 대한 영향도 최소화
2. 동일한 에러 알림 최적화
3. 완전 실시간 알림

### 성과

- 장애 감지 및 대응 시간이 단축됨
- 불필요한 알림 감소로 인한 운영 부담 완화
- 시스템 가용성 및 안정성 향상

### 회고

- 반복 알림을 줄이기 위해서 캐시를 사용했는데, 이게 최선의 방법이었을까?
- 반복 알림을 구분하기 위해서 몇개의 필드를 묶어 해시값으로 변환하여 사용했는데, 지금 생각해보면 IP 주소는 해시값에 넣지 않아도 될 것 같은데 왜 넣었을까?
- 완벽하게 동일한 알림인지 구분할 수 있는 방법은 없을까?
- 캐시에 들어간 에러들을 WriteBack 패턴을 사용해서 주기적으로 RDB로 동기화시키고 있는데, RDB에 있는 데이터를 추후에 어떤 방식으로 활용하면 좋을까?

상세내용: https://medium.com/lotteon-tech-blog/인터페이스-이력-시스템-개발기-a5a14a45ec76
## 인터페이스 이력 관리 시스템

### 기존 시스템 한계

MSA 환경에서 고객센터 모듈은 인터페이스 단위의 장애 및 결함 발생 시 원인을 신속하게 파악하기 어렵고, 인터페이스 요청 빈도 및 처리 시간에 대한 체계적인 분석이 부족해 성능 최적화에 한계가 있었습니다. 이는 고객 응대의 지연과 불만족으로 이어질 수 있는 문제였습니다.

이러한 문제를 해결하기 위해 AOP와 MDC를 활용한 인터페이스 이력 관리 시스템을 도입하였습니다. 이를 통해 인터페이스 요청 빈도와 처리 시간을 체계적으로 분석하고, 결함 및 장애 대응 시간을 단축하여 보다 신속한 고객 응대를 가능하게 했습니다.

### 개선점 도출

기존에 구축되어있는 주문/클레임 IF 이력 관리 시스템의 아쉬운 점을 나열하면 다음과 같았다.

- 사용자를 고려하지 않는 UI/UX 
- 과도한 스토리지 사용(개별 IF 이력 모두 DB에 레코드로 저장) 
- 과도한 I/O 요청(개별 IF 이력 모두 DB에 레코드로 저장) 
- 동일한 요청에도 불구하고 다른 추적ID(추적이 불가능) 

이러한 문제점을 바탕으로 다음과 같은 개선점을 도출하였다. 

- 개발자 친화적인 UI/UX 
- 합리적인 스토리지 사용 및 I/O 요청(하나의 요청에 대해서는 한 레코드에 저장) 
- 동일한 요청은 같은 추적ID, 구간ID는 스팬ID로 구분(하나의 요청인데 여러 MSA를 거치는 경우 각 MSA에는 같은 추적ID, 다른 스팬ID가 저장)

### 성과

- 인터페이스 요청 처리 속도 및 효율성 개선
- 시스템 병목 구간을 파악하고 최적화하여 성능 향상
- 장애 대응 속도 개선 및 시스템 안정성 강화

### 회고

- 인터페이스 이력에 대해서 요청을 구분하기 위해서 추적ID, 스팬ID로 구분하는 로직을 구현하였는데, 불필요한 인터페이스 이력이 너무 많이 쌓일 수 있어서 블랙리스트로 막아두었는데, 그렇다면 추적ID, 스팬ID가 완벽하게 제 역할을 하지 못하는 것이 아닐까?
- 어디까지 이력을 남겨야할까? 또한 화이트리스트로 이력을 관리하는게 좋을까? 아니면 블랙리스트로 관리하는 게 좋을까?
- 애매하게 중요하지만 너무 과도하게 많이 생기는 이력 데이터를 쌓는게 좋을까? 아니면 어떻게 하는게 좋을까?

상세내용: https://medium.com/lotteon-tech-blog/에러-1초-만에-잡아드립니다-aop와-redis로-구축한-실시간-알림-시스템-a80ca6910d9e

## 채팅 데이터 파이프라인 전환

### 기존 시스템 한계

고객센터 상담 채팅 솔루션을 제공하는 파트너사는 기존에 RabbitMQ를 통해 채팅 데이터를 고객센터 DB에 연동하고 있었으나, 전사 기술 표준화 과제에 따라 Kafka 기반 구조로의 전환이 요구되었습니다. 이에 따라 파트너사는 개발 공수를 요청하였고, 비용 절감을 위해 내부에서 직접 연동 개발을 진행하기로 결정하였습니다. 내부 시스템과 외부 DB 간의 데이터 구조가 상이한 상황에서도 Kafka Connect을 활용한 실시간 데이터 수집 파이프라인을 구축하여, 외부 채팅 데이터를 안정적이고 유연하게 내부 DB에 반영할 수 있는 구조를 완성하였습니다. 이를 통해 파트너사의 개발 공수 부담을 해소하고, 전사 표준에 부합하는 확장 가능한 연동 체계를 마련하였습니다.

### 개선점 도출

Kafka 환경에서 채팅 데이터 파이프라인 설계 및 구축

- 실시간 데이터 처리를 고려한 확장 가능한 Kafka 파이프라인 구조 설계
- ksqldb streams를 사용하여 불필요한 데이터를 필터링하고, 필요한 데이터만 전달하여 서버 부하를 감소시킴

PostgreSQL Debezium Connector 도입 및 레퍼런스 구축

- PostgreSQL CDC 복제를 위한 Debezium Connector를 분석 및 적용하여 데이터 처리 효율화
- 레퍼런스를 구축하고 문서화하여 향후 다른 프로젝트에서 재사용 가능하도록 가이드 정리

CDC 복제 시 메타데이터 관리 최적화

- 데일리 및 준실시간 모니터링 시스템을 구축하여 과용량 방지 및 운영 안정성 확보

### 성과

- Kafka 기반 실시간 데이터 처리로 성능 향상
- 데이터 필터링 최적화로 서버 부하 감소
- CDC 복제 효율화로 시스템 확장성 강화

### 회고

- Kafka Connect를 사용하는 것이 최선의 방법이었을까? 다른 방식은 없었을까?
- PostgreSQL 의 replication_slot을 사용하면 무조건 데이터 손실이 일어나지 않을까? 데이터 손실의 위험이 있다고 판단해서 DLQ를 두었는데, DLQ에 들어가기 전에 에러가 난다면? 구현 시간이 부족했더라도 재처리 로직을 구현해야 했을까?
- 채팅 솔루션을 사용했기에 미처 생각하지 못했었는데, 왜 채팅 솔루션 업체는 채팅 DB를 PostgreSQL을 사용했을까? PostgreSQL이 채팅과 같은 실시간 시스템에 MySQL이나 NOSQL 보다 최적화 되어있을까?

## 쿼리 튜닝(VOC 기처리 배치 쿼리, 판매자 연락 건수 조회)

### 기존 방식의 한계

기존 VOC 기처리 배치 쿼리는 데이터가 많아지면서 내부적인 실행계획의 변경으로 인덱스를 타지 않게 되어서 풀스캔을 하게되어 길면 1시간 정도 걸리는 쿼리가 실행되었습니다. 이에 쿼리를 튜닝하여 인덱스를 타게끔 하거나 쿼리를 변경하여 쿼리 실행 시간을 단축하고자 했습니다.

기존 판매자 연락 건수 조회 쿼리는 조건문에 OR 로 두 개의 기간 조건을 타게 되면서 실행계획 상에서는 인덱스를 타는 것처럼 보이지만 실제로 인덱스를 타지 않는 경우가 발생하였습니다. 이에 쿼리를 튜닝하여 인덱스를 타게끔 하거나 쿼리를 변경하여 쿼리 실행 시간을 단축하고자 했습니다.

### 개선점 도출

VOC 배치 쿼리에 인덱스를 타게 하려고 인덱스를 추가하려고 했으나, 테이블에 이미 과도한 인덱스가 존재해서 인덱스 추가는 불가능했습니다. 따라서 기존 인덱스를 타게끔 하려고 인덱스 힌트를 추가하여서 강제로 인덱스를 타게 하였습니다.

판매자 연락 건수 조회 쿼리는 각각의 일자 조건문을 분리하여 UNION ALL 로 두 개의 쿼리를 묶었고, 각각의 쿼리가 인덱스를 타도록 강제하였습니다.

### 성과

- VOC 배치 쿼리 1시간 -> 5분 내외로 성능 개선
- 판매자 연락 건수 조회 쿼리를 인덱스 타도록 유도하여 1분 내외로 성능 개선

### 회고

- 테이블의 인덱스가 이미 너무 많아서 인덱스 힌트를 사용해서 인덱스를 타도록 실행계획을 강제했는데, 이게 최선이었을까?
- 인덱스 힌트를 사용하면 좋지 않다고 하는데 정확히 어떤점이 안좋을까? 인덱스 힌트나 인덱스 추가 말고는 다른 방법은 없을까?
- OR 조건으로 날짜가 각각 있어서 인덱스를 제대로 타지 않을 수 있어서 UNION ALL로 두 개의 쿼리로 나눠서 각각의 쿼리가 인덱스를 타도록 했는데, 이게 최선이었을까? 다른 더 최적의 방식은 없었을까?